# ğŸš€ LiteLLM Connector for GitHub Copilot Chat

[![CI](https://github.com/gethnet/litellm-connector-copilot/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/gethnet/litellm-connector-copilot/actions/workflows/ci.yml)
[![Codecov](https://codecov.io/gh/gethnet/litellm-connector-copilot/branch/main/graph/badge.svg)](https://codecov.io/gh/gethnet/litellm-connector-copilot)
[![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/gethnet/litellm-connector-copilot?sort=semver)](https://github.com/gethnet/litellm-connector-copilot/releases)
[![VS Code Marketplace Version](https://img.shields.io/visual-studio-marketplace/v/GethNet.litellm-connector-copilot)](https://marketplace.visualstudio.com/items?itemName=GethNet.litellm-connector-copilot)
[![VS Code Marketplace Installs](https://img.shields.io/visual-studio-marketplace/i/GethNet.litellm-connector-copilot)](https://marketplace.visualstudio.com/items?itemName=GethNet.litellm-connector-copilot)

Bring **any LiteLLM-supported model** into the Copilot Chat model picker â€” OpenAI, Anthropic (Claude), Google, Mistral, local Llama, and more.

If LiteLLM can talk to it, **Copilot can use it**.

---

## âœ… Requirements

- ğŸ”‘ **GitHub Copilot** subscription (Free plan works)
- ğŸŒ A **LiteLLM proxy URL** (and an API key if your proxy requires one)

---

## âš¡ Quick Start (60 seconds)

1. Install **GitHub Copilot Chat**
2. Install **LiteLLM Connector for Copilot**
3. Open Command Palette: `Ctrl+Shift+P` / `Cmd+Shift+P`
4. Run: **Manage LiteLLM Provider**
5. Enter:
   - **Base URL** (example: `http://localhost:4000`)
   - **API Key** (optional)
6. Open Copilot Chat â†’ pick a model under **LiteLLM** â†’ chat ğŸ‰

---

## âœ¨ What you get

- ğŸŒ **Hundreds of models** via your LiteLLM proxy
- ğŸŒŠ **Real-time streaming** responses
- ğŸ› ï¸ **Tool / function calling** support
- ğŸ‘ï¸ **Vision models** supported (where available)
- ğŸ§  **Smart parameter handling** for model quirks
- ğŸ” **Automatic retry** when a model rejects unsupported flags
- â±ï¸ **Inactivity watchdog** to prevent stuck streams
- ğŸš«ğŸ§  **Cache bypass controls** (`no-cache` headers) with provider-aware behavior
- ğŸ” **Secure credential storage** using VS Code `SecretStorage`

---

## ğŸ†• Recent Highlights

- ğŸš€ **VS Code 1.109+ settings modernization** (aligns with the Language Model provider settings UI)
- ğŸ“¦ **Smaller, faster package** (bundled/minified production builds)
- ğŸŒ **Web-ready output** (includes a browser-target bundle for VS Code Web)

---

## âš™ï¸ Configuration

- `litellm-connector.inactivityTimeout` *(number, default: 60)*
  - Seconds of inactivity before the LiteLLM connection is considered idle.
- `litellm-connector.disableCaching` *(boolean, default: true)*
  - Sends `no-cache: true` and `Cache-Control: no-cache` to bypass LiteLLM caching.

---

## âŒ¨ï¸ Commands

- **Manage LiteLLM Provider**: Configure Base URL + API Key; refreshes models.

---

## ğŸ§© Notes

- This extension is a **provider** for the official Copilot Chat experience.
- It wonâ€™t function without the **GitHub Copilot Chat** extension installed.

---

## ğŸ†˜ Support

- Issues & feedback: https://github.com/gethnet/litellm-connector-copilot/issues
- License: Apache-2.0
